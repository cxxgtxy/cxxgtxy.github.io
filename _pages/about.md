---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Senior Director, AMAP, Alibaba Group
======

I am a senior director at Alibaba (AMAP), leading a team that delivers intelligent route navigation and implements AIGC solutions for core business scenarios. In 2023, I was fortunate to be selected as one of the **Top 100 AI Scholars** by AMiner. Prior to that, I served as a senior manager at Meituan (2020-2024) and Xiaomi (2017-2020). I earned my Bachelor of Science in Electrical Engineering from Southeast University in 2010, and later my Master of Science in Electrical Engineering from Tsinghua University in 2012.

*We are always looking for talented interns/full-time researchers with strong coding skills and research experience. Please drop me an email if interested.*

---

Highlights
------

### Research & Innovation
- Published **50+ papers** at top AI conferences, with **20 first-authored papers**
- **3 papers** selected for PaperDigest's Most Influential Paper List: *FairNAS*, *Twins*, *CPVT*
- Holds **40+ domestic** and **7 international** invention patents

### Research Directions
LLM Pre-training | Multimodal Large Models | Reinforcement Learning | Generative Models | Foundation Model Design | Neural Architecture Search | Model Compression | Self-supervised Learning | 2D/3D Perception

### Industry Impact
- Rich implementation experience across **AI platforms**, **autonomous driving**, and **edge-cloud systems**
- Excels at creating value through technology and overcoming complex challenges

### Leadership
- Managed technical teams of **100+ members**
- Over 50% of team members recruited from renowned domestic and international AI labs
- Open-sourced industry-leading detection framework [YOLOv6](https://github.com/meituan/YOLOv6)

### Recognition
- Multiple industry invited talks and top conference presentations
- Technology media coverage and public recognition from Lei Jun & Xiaomi

---

Work Experience
======

### Alibaba Group - AMAP
**Senior Director** | Mar 2024 - Present

*Team Direction:* Multi-modal Large Models, Generation Models, Route Navigation, Recommendation, Spatio-temporal Data Mining

---

### Meituan - Visual Intelligence Department
**Senior Technical Manager** | May 2020 - Mar 2024

*Team Direction:* Multi-modal Large Models, Large Language Model, Perception (2D+3D), Foundation Model, Self-supervised Pre-training, Model Compression, MLOps

**Large Foundation Models (LLM and Multi-modal)**
- Built general understanding (multi-modal) large model with open set understanding capabilities for Meituan scenarios
- Achieved industry-leading compression and efficient deployment of the company's trillion-parameter LLM
- Trained and deployed LLM base model for embodied intelligence scenarios
- Reproduced Meta LLaMA 7B from scratch; developed 1B and 3B [MobileVLM](https://github.com/Meituan-AutoML/MobileVLM) benchmarking Gemini Nano
- Created [VisionLLaMA](https://github.com/Meituan-AutoML/VisionLLaMA) unifying vision and language architecture; [Twins](https://github.com/Meituan-AutoML/Twins) outperforming Swin with production deployment advantages

**Autonomous Driving & Drones**
- Built 3D offline system for autonomous delivery vehicles, saving millions in annotation costs annually
- Improved online perception algorithms (3D obstacle detection, tracking) achieving industry-leading real-time capabilities
- Launched core perception modules on third and fourth generation drones

**AI Business Support & MLOps**
- Constructed vision code base and AutoML tools
- Model compression and deployment (end, edge, cloud) covering face recognition, OCR, security review, image understanding
- Saved millions in vision service costs annually

**Team & Influence Building**
- Built team from 0 to 1; recruited multiple Google PhD Fellowship recipients
- Open-sourced and maintained [YOLOv6](https://github.com/meituan/YOLOv6) detection library
- Contributed to National Science and Technology Innovation 2030 major project on AI fundamental models

---

### Xiaomi - Artificial Intelligence Department
**Senior Technical Manager** | Mar 2017 - May 2020

*Team Direction:* AutoML, Base Model Design, Machine Learning

- Initiated AutoML project and built the team from 0 to 1
- Implemented in scene recognition, segmentation, acoustic scene classification, recommendation
- Won **2nd place** in Xiaomi's first **"Million Dollar Prize"** (Automated Neural Network Design)

---

### Beijing KingStar System Control Co., Ltd.
**Deputy Director** | Jun 2013 - Mar 2017

- Key contributor to "Complex Power Grid Autonomy - Collaborative Automatic Voltage Control" project
- Awarded **2018 National Science and Technology Progress First Prize** (contributed 20 invention patents)

---

### IBM Research China (CRL)
**Research Scientist** | Jul 2012 - May 2013

---

Publications
======

### First-Author Papers
<ol>
<small>
  <li>Twins: Revisiting the design of spatial attention in vision transformers, NeurIPS21</li>
  <li>Conditional Positional Encodings for Vision Transformers, ICLR23</li>
  <li>VisionLLaMA: A Unified LLaMA Backbone for Vision Tasks, ECCV24</li>
  <li>Gpg: A simple and strong reinforcement learning baseline for model reasoning, ICLR26</li>
  <li>MobileVLM: A Fast, Strong and Open Vision Language Assistant for Mobile Devices</li>
  <li>MobileVLM V2: Faster and Stronger Baseline for Vision Language Model</li>
  <li>FairNAS: Rethinking evaluation fairness of weight sharing neural architecture search, ICCV21</li>
  <li>Fair DARTS: Eliminating unfair advantages in differentiable architecture search, ECCV20</li>
  <li>DARTS-: Robustly stepping out of performance collapse without indicators, ICLR21</li>
  <li>ROME: Robustifying memory-efficient NAS via topology disentanglement and gradients accumulation, ICCV23</li>
  <li>Make RepVGG Greater Again: A Quantization-aware Approach, AAAI24</li>
  <li>MixPATH: A unified approach for one-shot neural architecture search, ICCV23</li>
  <li>USP: Unified self-supervised pretraining for image generation and understanding, ICCV25</li>
  <li>Noisy differentiable architecture search, BMVC21</li>
  <li>A Unified Mixture-View Framework for Unsupervised Representation Learning, BMVC22</li>
  <li>Multi-objective reinforced evolution in mobile neural architecture search, ECCVW2020</li>
  <li>Fast, accurate and lightweight super-resolution with neural architecture search, ICPR20</li>
  <li>MoGA: Searching beyond MobileNetV3, ICASSP2020</li>
  <li>Scarlet-NAS: Bridging the gap between stability and scalability in weight-sharing NAS, ICCVW21</li>
  <li>Parameter sharing deep deterministic policy gradient for cooperative multi-agent reinforcement learning</li>
  <li>Improved crowding distance for NSGA-II</li>
  <li>Policy optimization with penalized point probability distance: An alternative to PPO</li>
</small>
</ol>

### Collaborative Papers
<ol>
<small>
  <li>Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation, ICLR26</li>
  <li>Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models, ICLR26</li>
  <li>There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training, ICLR26</li>
  <li>Video-star: Reinforcing open-vocabulary action recognition with tools, ICLR26</li>
  <li>Tree search for LLM agent reinforcement learning, ICLR26</li>
  <li>AutoDrive-R: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving, ICLR26</li>
  <li>S-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models, ICLR26</li>
  <li>Narrlv: Towards a comprehensive narrative-centric evaluation for long video generation models, ICLR26</li>
  <li>Where and What Matters: Sensitivity-Aware Task Vectors for Many-Shot Multimodal In-Context Learning, AAAI26</li>
  <li>Scalar: Scale-wise controllable visual autoregressive learning, AAAI26</li>
  <li>Omni-effects: Unified and spatially-controllable visual effects generation, AAAI26</li>
  <li>AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting, AAAI26</li>
  <li>ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints, AAAI26</li>
  <li>Position bias mitigates position bias: Mitigate position bias through inter-position knowledge distillation, EMNLP25 oral</li>
  <li>HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation, EMNLP25 oral</li>
  <li>UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement, ICCV25</li>
  <li>VMBench: A Benchmark for Perception-Aligned Video Motion Generation, ICCV25</li>
  <li>LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling, ICCV25</li>
  <li>FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos, ACM MM25</li>
  <li>Dyn-Adapter: Towards Disentangled Representation for Efficient Visual Recognition, ECCV24</li>
  <li>Revealing the Dark Secrets of Extremely Large Kernel ConvNets on Robustness, ICML24</li>
  <li>PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution, CVPR24</li>
  <li>LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection, ICLR24</li>
  <li>Norm Tweaking: High-performance Low-bit Quantization of Large Language Models, AAAI24</li>
  <li>YOLOv6: A single-stage object detection framework for industrial applications (5.3k GitHub stars)</li>
  <li>A Speed Odyssey for Deployable Quantization of LLMs</li>
  <li>FPTQ: Fine-grained Post-Training Quantization for Large Language Models</li>
  <li>Lenna: Language Enhanced Reasoning Detection Assistant</li>
  <li>SCTNet: Single Branch CNN with Transformer Semantic Information for Real-time Segmentation, AAAI24</li>
  <li>Promptdet: Towards open-vocabulary detection using uncurated images, ECCV22</li>
  <li>SegVIT: Semantic segmentation with plain vision transformers, NeurIPS22</li>
  <li>Fully convolutional one-stage 3D object detection on LiDAR range images, NeurIPS22</li>
  <li>Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation, CVPR22</li>
  <li>Aedet: Azimuth-invariant multi-view 3D object detection, CVPR23</li>
  <li>EAPruning: Evolutionary Pruning for Vision Transformers and CNNs, BMVC22</li>
  <li>AutoKWS: Keyword Spotting with Differentiable Architecture Search, ICASSP21</li>
  <li>Neural Architecture Search on Acoustic Scene Classification, InterSpeech20</li>
  <li>Accurate and efficient single image super-resolution with matrix channel attention network, ACCV20</li>
  <li>STRETCH meat grinder with ICCOS, IEEE Transactions on Plasma Science</li>
  <li>Comparisons of three inductive pulse power supplies, IEEE Transactions on Plasma Science</li>
  <li>FastPillars: A Deployment-friendly Pillar-based 3D Detector</li>
  <li>LogicalDefender: Discovering, Extracting, and Utilizing Common-Sense Knowledge</li>
</small>
</ol>

---

Selected Media Coverage
======

1. [全面超越ViT，美团、浙大等提出视觉任务统一架构VisionLLAMA](https://mp.weixin.qq.com/s/grFI_fapRtjVp7CbPkfTNA)
2. [端侧实时运行、3B媲美7B！美团、浙大等提出MobileVLM V2：更快、更强的端侧视觉语言模型](https://mp.weixin.qq.com/s/WPLWmxkjlc6_2sn8ToHBqg)
3. [骁龙888实时运行，美团、浙大等打造全流程移动端多模态大模型MobileVLM](https://mp.weixin.qq.com/s/-KnewDBeCN7a1XPk22u9Pw)
4. [美团提出基于隐式条件位置编码的Transformer，性能优于 ViT 和 DeiT](https://www.jiqizhixin.com/articles/2021-02-26)
5. [Twins：重新思考高效的视觉注意力模型设计](https://tech.meituan.com/2022/03/24/twins-revisiting-the-design-of-spatial-attention-in-vision-transformers.html)
6. [更准更快的YOLOv6来了，美团出品并开源](https://www.jiqizhixin.com/articles/2022-06-26-2)
7. [小米AI实验室成果速递](https://weibo.com/1771925961/I3RJIzzj0)
8. [雷军强推:小米造最强超分辨率算法，现已开源](https://www.jiqizhixin.com/articles/2019-02-20-14)
9. [超越MnasNet、Proxyless：小米提出全新神经架构搜索算法FairNAS](https://www.jiqizhixin.com/articles/2019-07-05-6)
10. [两个月三项成果，对标谷歌！独家对话小米AutoML团队，如何让模型搜索更公平](https://mp.weixin.qq.com/s/lMLAd2sTZdRjbMa38HS3vg)


 <a href='http://www.freevisitorcounters.com'>visitorcounters</a> <script type='text/javascript' src='https://www.freevisitorcounters.com/auth.php?id=31dd2b5ea9e0f23b78cdb124346f25d08819f711'></script>
<script type="text/javascript" src="https://www.freevisitorcounters.com/en/home/counter/1458162/t/1"></script>
