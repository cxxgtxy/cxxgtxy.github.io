---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

{% include base_path %}

You can also find my articles on <u><a href="https://scholar.google.com/citations?user=jn21pUsAAAAJ&hl=zh-CN">my Google Scholar profile</a></u>.

---

First-Author Papers
======

<ol>
  <li><a href="https://arxiv.org/abs/2104.13840">Twins: Revisiting the design of spatial attention in vision transformers</a>, <strong>NeurIPS21</strong> <a href="https://github.com/Meituan-AutoML/Twins"><img src="https://img.shields.io/github/stars/Meituan-AutoML/Twins?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2102.10882">Conditional Positional Encodings for Vision Transformers</a>, <strong>ICLR23</strong></li>
  <li><a href="https://arxiv.org/abs/2403.00522">VisionLLaMA: A Unified LLaMA Backbone for Vision Tasks</a>, <strong>ECCV24</strong> <a href="https://github.com/Meituan-AutoML/VisionLLaMA"><img src="https://img.shields.io/github/stars/Meituan-AutoML/VisionLLaMA?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2504.02546">GPG: A simple and strong reinforcement learning baseline for model reasoning</a>, <strong>ICLR26</strong> <a href="https://github.com/AMAP-ML/GPG"><img src="https://img.shields.io/github/stars/AMAP-ML/GPG?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2312.16886">MobileVLM: A Fast, Strong and Open Vision Language Assistant for Mobile Devices</a> <a href="https://github.com/Meituan-AutoML/MobileVLM"><img src="https://img.shields.io/github/stars/Meituan-AutoML/MobileVLM?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2402.03766">MobileVLM V2: Faster and Stronger Baseline for Vision Language Model</a></li>
  <li><a href="https://arxiv.org/abs/1907.01845">FairNAS: Rethinking evaluation fairness of weight sharing neural architecture search</a>, <strong>ICCV21</strong> <a href="https://github.com/xiaomi-automl/FairNAS"><img src="https://img.shields.io/github/stars/xiaomi-automl/FairNAS?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/1911.12126">Fair DARTS: Eliminating unfair advantages in differentiable architecture search</a>, <strong>ECCV20</strong></li>
  <li><a href="https://arxiv.org/abs/2009.01027">DARTS-: Robustly stepping out of performance collapse without indicators</a>, <strong>ICLR21</strong> <a href="https://github.com/Meituan-AutoML/DARTS-"><img src="https://img.shields.io/github/stars/Meituan-AutoML/DARTS-?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2011.11233">ROME: Robustifying memory-efficient NAS via topology disentanglement and gradients accumulation</a>, <strong>ICCV23</strong></li>
  <li><a href="https://arxiv.org/abs/2212.01593">Make RepVGG Greater Again: A Quantization-aware Approach</a>, <strong>AAAI24</strong> <a href="https://github.com/cxxgtxy/QARepVGG"><img src="https://img.shields.io/github/stars/cxxgtxy/QARepVGG?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2001.05887">MixPATH: A unified approach for one-shot neural architecture search</a>, <strong>ICCV23</strong></li>
  <li><a href="https://arxiv.org/abs/2503.06132">USP: Unified self-supervised pretraining for image generation and understanding</a>, <strong>ICCV25</strong> <a href="https://github.com/AMAP-ML/USP"><img src="https://img.shields.io/github/stars/AMAP-ML/USP?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2005.03566">Noisy differentiable architecture search</a>, <strong>BMVC21</strong></li>
  <li><a href="https://arxiv.org/abs/2011.13356">A Unified Mixture-View Framework for Unsupervised Representation Learning</a>, <strong>BMVC22</strong></li>
  <li><a href="https://arxiv.org/abs/1901.01074">Multi-objective reinforced evolution in mobile neural architecture search</a>, <strong>ECCVW2020</strong></li>
  <li><a href="https://arxiv.org/abs/1901.07261">Fast, accurate and lightweight super-resolution with neural architecture search</a>, <strong>ICPR20</strong> <a href="https://github.com/xiaomi-automl/FALSR"><img src="https://img.shields.io/github/stars/xiaomi-automl/FALSR?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://ieeexplore.ieee.org/document/9054428">MoGA: Searching beyond MobileNetV3</a>, <strong>ICASSP2020</strong> <a href="https://github.com/xiaomi-automl/MoGA"><img src="https://img.shields.io/github/stars/xiaomi-automl/MoGA?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/1908.06022">Scarlet-NAS: Bridging the gap between stability and scalability in weight-sharing NAS</a>, <strong>ICCVW21</strong></li>
  <li><a href="https://arxiv.org/abs/1710.00336">Parameter sharing deep deterministic policy gradient for cooperative multi-agent reinforcement learning</a></li>
  <li><a href="https://arxiv.org/abs/1811.12667">Improved crowding distance for NSGA-II</a></li>
  <li><a href="https://arxiv.org/abs/1807.00442">Policy optimization with penalized point probability distance: An alternative to PPO</a></li>
</ol>

---

Collaborative Papers
======

<ol>
  <li><a href="https://arxiv.org/abs/2601.20614">Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation</a>, <strong>ICLR26</strong> <a href="https://github.com/AMAP-ML/MathForge"><img src="https://img.shields.io/github/stars/AMAP-ML/MathForge?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2601.20354">Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models</a>, <strong>ICLR26</strong> <a href="https://github.com/AMAP-ML/SpatialGenEval"><img src="https://img.shields.io/github/stars/AMAP-ML/SpatialGenEval?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2510.12586">There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training</a>, <strong>ICLR26</strong> <a href="https://github.com/AMAP-ML/EPG"><img src="https://img.shields.io/github/stars/AMAP-ML/EPG?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2510.08480">Video-star: Reinforcing open-vocabulary action recognition with tools</a>, <strong>ICLR26</strong></li>
  <li><a href="https://arxiv.org/abs/2509.21240">Tree search for LLM agent reinforcement learning</a>, <strong>ICLR26</strong> <a href="https://github.com/AMAP-ML/Tree-GRPO"><img src="https://img.shields.io/github/stars/AMAP-ML/Tree-GRPO?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2509.01944">AutoDrive-R: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving</a>, <strong>ICLR26</strong></li>
  <li><a href="https://arxiv.org/abs/2508.12880">S-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models</a>, <strong>ICLR26</strong> <a href="https://github.com/AMAP-ML/S2-Guidance"><img src="https://img.shields.io/github/stars/AMAP-ML/S2-Guidance?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2507.00790">Narrlv: Towards a comprehensive narrative-centric evaluation for long video generation models</a>, <strong>ICLR26</strong> <a href="https://github.com/AMAP-ML/NarrLV"><img src="https://img.shields.io/github/stars/AMAP-ML/NarrLV?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2511.08246">Where and What Matters: Sensitivity-Aware Task Vectors for Many-Shot Multimodal In-Context Learning</a>, <strong>AAAI26</strong></li>
  <li><a href="https://arxiv.org/abs/2507.19946">Scalar: Scale-wise controllable visual autoregressive learning</a>, <strong>AAAI26</strong> <a href="https://github.com/AMAP-ML/SCALAR"><img src="https://img.shields.io/github/stars/AMAP-ML/SCALAR?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2508.07981">Omni-effects: Unified and spatially-controllable visual effects generation</a>, <strong>AAAI26</strong> <a href="https://github.com/AMAP-ML/Omni-Effects"><img src="https://img.shields.io/github/stars/AMAP-ML/Omni-Effects?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2511.09478">AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting</a>, <strong>AAAI26</strong></li>
  <li><a href="https://arxiv.org/abs/2510.14847">ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints</a>, <strong>AAAI26</strong> <a href="https://github.com/AMAP-ML/ImagerySearch"><img src="https://img.shields.io/github/stars/AMAP-ML/ImagerySearch?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2508.15709">Position bias mitigates position bias: Mitigate position bias through inter-position knowledge distillation</a>, <strong>EMNLP25 oral</strong> <a href="https://github.com/AMAP-ML/Pos2Distill"><img src="https://img.shields.io/github/stars/AMAP-ML/Pos2Distill?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2505.19866">HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation</a>, <strong>EMNLP25 oral</strong> <a href="https://github.com/AMAP-ML/HS-STaR"><img src="https://img.shields.io/github/stars/AMAP-ML/HS-STaR?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2507.00721">UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement</a>, <strong>ICCV25</strong> <a href="https://github.com/AMAP-ML/UPRE"><img src="https://img.shields.io/github/stars/AMAP-ML/UPRE?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2507.00790">VMBench: A Benchmark for Perception-Aligned Video Motion Generation</a>, <strong>ICCV25</strong> <a href="https://github.com/AMAP-ML/VMBench"><img src="https://img.shields.io/github/stars/AMAP-ML/VMBench?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2507.00790">LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling</a>, <strong>ICCV25</strong> <a href="https://github.com/AMAP-ML/LD-RPS"><img src="https://img.shields.io/github/stars/AMAP-ML/LD-RPS?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2504.10358">FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos</a>, <strong>ACM MM25</strong> <a href="https://github.com/AMAP-ML/FingER"><img src="https://img.shields.io/github/stars/AMAP-ML/FingER?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2407.14302">Dyn-Adapter: Towards Disentangled Representation for Efficient Visual Recognition</a>, <strong>ECCV24</strong></li>
  <li><a href="https://arxiv.org/abs/2407.08972">Revealing the Dark Secrets of Extremely Large Kernel ConvNets on Robustness</a>, <strong>ICML24</strong></li>
  <li><a href="https://arxiv.org/abs/2403.07589">PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution</a>, <strong>CVPR24</strong></li>
  <li><a href="https://arxiv.org/abs/2401.15865">LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection</a>, <strong>ICLR24</strong></li>
  <li><a href="https://arxiv.org/abs/2309.02784">Norm Tweaking: High-performance Low-bit Quantization of Large Language Models</a>, <strong>AAAI24</strong></li>
  <li><a href="https://arxiv.org/abs/2209.02976">YOLOv6: A single-stage object detection framework for industrial applications</a>, <strong>arXiv</strong> <a href="https://github.com/meituan/YOLOv6"><img src="https://img.shields.io/github/stars/meituan/YOLOv6?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2311.09550">A Speed Odyssey for Deployable Quantization of LLMs</a></li>
  <li><a href="https://arxiv.org/abs/2308.15987">FPTQ: Fine-grained Post-Training Quantization for Large Language Models</a></li>
  <li><a href="https://arxiv.org/abs/2312.02433">Lenna: Language Enhanced Reasoning Detection Assistant</a> <a href="https://github.com/Meituan-AutoML/Lenna"><img src="https://img.shields.io/github/stars/Meituan-AutoML/Lenna?style=social" alt="GitHub stars"></a></li>
  <li><a href="https://arxiv.org/abs/2312.17071">SCTNet: Single Branch CNN with Transformer Semantic Information for Real-time Segmentation</a>, <strong>AAAI24</strong></li>
  <li><a href="https://arxiv.org/abs/2203.16513">PromptDet: Towards open-vocabulary detection using uncurated images</a>, <strong>ECCV22</strong></li>
  <li><a href="https://arxiv.org/abs/2210.05844">SegViT: Semantic segmentation with plain vision transformers</a>, <strong>NeurIPS22</strong></li>
  <li><a href="https://arxiv.org/abs/2205.13764">Fully convolutional one-stage 3D object detection on LiDAR range images</a>, <strong>NeurIPS22</strong></li>
  <li><a href="https://arxiv.org/abs/2204.02547">Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation</a>, <strong>CVPR22</strong></li>
  <li><a href="https://arxiv.org/abs/2211.12501">AeDet: Azimuth-invariant multi-view 3D object detection</a>, <strong>CVPR23</strong></li>
  <li><a href="https://arxiv.org/abs/2210.00181">EAPruning: Evolutionary Pruning for Vision Transformers and CNNs</a>, <strong>BMVC22</strong></li>
  <li><a href="https://arxiv.org/abs/2009.03658">AutoKWS: Keyword Spotting with Differentiable Architecture Search</a>, <strong>ICASSP21</strong></li>
  <li>Neural Architecture Search on Acoustic Scene Classification, <strong>InterSpeech20</strong></li>
  <li><a href="https://openaccess.thecvf.com/content/ACCV2020/html/Ma_Accurate_and_Efficient_Single_Image_Super-Resolution_with_Matrix_Channel_Attention_ACCV_2020_paper.html">Accurate and efficient single image super-resolution with matrix channel attention network</a>, <strong>ACCV20</strong></li>
  <li>STRETCH meat grinder with ICCOS, <strong>IEEE Transactions on Plasma Science</strong></li>
  <li>Comparisons of three inductive pulse power supplies, <strong>IEEE Transactions on Plasma Science</strong></li>
  <li><a href="https://arxiv.org/abs/2302.02367">FastPillars: A Deployment-friendly Pillar-based 3D Detector</a></li>
</ol>
