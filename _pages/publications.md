---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---

{% include base_path %}

You can also find my articles on <u><a href="https://scholar.google.com/citations?user=jn21pUsAAAAJ&hl=zh-CN">my Google Scholar profile</a></u>.

---

First-Author Papers
======

<ol>
  <li>Twins: Revisiting the design of spatial attention in vision transformers, <strong>NeurIPS21</strong></li>
  <li>Conditional Positional Encodings for Vision Transformers, <strong>ICLR23</strong></li>
  <li>VisionLLaMA: A Unified LLaMA Backbone for Vision Tasks, <strong>ECCV24</strong></li>
  <li>Gpg: A simple and strong reinforcement learning baseline for model reasoning, <strong>ICLR26</strong></li>
  <li>MobileVLM: A Fast, Strong and Open Vision Language Assistant for Mobile Devices</li>
  <li>MobileVLM V2: Faster and Stronger Baseline for Vision Language Model</li>
  <li>FairNAS: Rethinking evaluation fairness of weight sharing neural architecture search, <strong>ICCV21</strong></li>
  <li>Fair DARTS: Eliminating unfair advantages in differentiable architecture search, <strong>ECCV20</strong></li>
  <li>DARTS-: Robustly stepping out of performance collapse without indicators, <strong>ICLR21</strong></li>
  <li>ROME: Robustifying memory-efficient NAS via topology disentanglement and gradients accumulation, <strong>ICCV23</strong></li>
  <li>Make RepVGG Greater Again: A Quantization-aware Approach, <strong>AAAI24</strong></li>
  <li>MixPATH: A unified approach for one-shot neural architecture search, <strong>ICCV23</strong></li>
  <li>USP: Unified self-supervised pretraining for image generation and understanding, <strong>ICCV25</strong></li>
  <li>Noisy differentiable architecture search, <strong>BMVC21</strong></li>
  <li>A Unified Mixture-View Framework for Unsupervised Representation Learning, <strong>BMVC22</strong></li>
  <li>Multi-objective reinforced evolution in mobile neural architecture search, <strong>ECCVW2020</strong></li>
  <li>Fast, accurate and lightweight super-resolution with neural architecture search, <strong>ICPR20</strong></li>
  <li>MoGA: Searching beyond MobileNetV3, <strong>ICASSP2020</strong></li>
  <li>Scarlet-NAS: Bridging the gap between stability and scalability in weight-sharing NAS, <strong>ICCVW21</strong></li>
  <li>Parameter sharing deep deterministic policy gradient for cooperative multi-agent reinforcement learning</li>
  <li>Improved crowding distance for NSGA-II</li>
  <li>Policy optimization with penalized point probability distance: An alternative to PPO</li>
</ol>

---

Collaborative Papers
======

<ol>
  <li>Harder Is Better: Boosting Mathematical Reasoning via Difficulty-Aware GRPO and Multi-Aspect Question Reformulation, <strong>ICLR26</strong></li>
  <li>Everything in Its Place: Benchmarking Spatial Intelligence of Text-to-Image Models, <strong>ICLR26</strong></li>
  <li>There is No VAE: End-to-End Pixel-Space Generative Modeling via Self-Supervised Pre-training, <strong>ICLR26</strong></li>
  <li>Video-star: Reinforcing open-vocabulary action recognition with tools, <strong>ICLR26</strong></li>
  <li>Tree search for LLM agent reinforcement learning, <strong>ICLR26</strong></li>
  <li>AutoDrive-R: Incentivizing Reasoning and Self-Reflection Capacity for VLA Model in Autonomous Driving, <strong>ICLR26</strong></li>
  <li>S-Guidance: Stochastic Self Guidance for Training-Free Enhancement of Diffusion Models, <strong>ICLR26</strong></li>
  <li>Narrlv: Towards a comprehensive narrative-centric evaluation for long video generation models, <strong>ICLR26</strong></li>
  <li>Where and What Matters: Sensitivity-Aware Task Vectors for Many-Shot Multimodal In-Context Learning, <strong>AAAI26</strong></li>
  <li>Scalar: Scale-wise controllable visual autoregressive learning, <strong>AAAI26</strong></li>
  <li>Omni-effects: Unified and spatially-controllable visual effects generation, <strong>AAAI26</strong></li>
  <li>AdaCuRL: Adaptive Curriculum Reinforcement Learning with Invalid Sample Mitigation and Historical Revisiting, <strong>AAAI26</strong></li>
  <li>ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints, <strong>AAAI26</strong></li>
  <li>Position bias mitigates position bias: Mitigate position bias through inter-position knowledge distillation, <strong>EMNLP25 oral</strong></li>
  <li>HS-STAR: Hierarchical Sampling for Self-Taught Reasoners via Difficulty Estimation and Budget Reallocation, <strong>EMNLP25 oral</strong></li>
  <li>UPRE: Zero-Shot Domain Adaptation for Object Detection via Unified Prompt and Representation Enhancement, <strong>ICCV25</strong></li>
  <li>VMBench: A Benchmark for Perception-Aligned Video Motion Generation, <strong>ICCV25</strong></li>
  <li>LD-RPS: Zero-Shot Unified Image Restoration via Latent Diffusion Recurrent Posterior Sampling, <strong>ICCV25</strong></li>
  <li>FingER: Content Aware Fine-grained Evaluation with Reasoning for AI-Generated Videos, <strong>ACM MM25</strong></li>
  <li>Dyn-Adapter: Towards Disentangled Representation for Efficient Visual Recognition, <strong>ECCV24</strong></li>
  <li>Revealing the Dark Secrets of Extremely Large Kernel ConvNets on Robustness, <strong>ICML24</strong></li>
  <li>PeLK: Parameter-efficient Large Kernel ConvNets with Peripheral Convolution, <strong>CVPR24</strong></li>
  <li>LiDAR-PTQ: Post-Training Quantization for Point Cloud 3D Object Detection, <strong>ICLR24</strong></li>
  <li>Norm Tweaking: High-performance Low-bit Quantization of Large Language Models, <strong>AAAI24</strong></li>
  <li>YOLOv6: A single-stage object detection framework for industrial applications (5.3k GitHub stars)</li>
  <li>A Speed Odyssey for Deployable Quantization of LLMs</li>
  <li>FPTQ: Fine-grained Post-Training Quantization for Large Language Models</li>
  <li>Lenna: Language Enhanced Reasoning Detection Assistant</li>
  <li>SCTNet: Single Branch CNN with Transformer Semantic Information for Real-time Segmentation, <strong>AAAI24</strong></li>
  <li>Promptdet: Towards open-vocabulary detection using uncurated images, <strong>ECCV22</strong></li>
  <li>SegVIT: Semantic segmentation with plain vision transformers, <strong>NeurIPS22</strong></li>
  <li>Fully convolutional one-stage 3D object detection on LiDAR range images, <strong>NeurIPS22</strong></li>
  <li>Modeling Motion with Multi-Modal Features for Text-Based Video Segmentation, <strong>CVPR22</strong></li>
  <li>Aedet: Azimuth-invariant multi-view 3D object detection, <strong>CVPR23</strong></li>
  <li>EAPruning: Evolutionary Pruning for Vision Transformers and CNNs, <strong>BMVC22</strong></li>
  <li>AutoKWS: Keyword Spotting with Differentiable Architecture Search, <strong>ICASSP21</strong></li>
  <li>Neural Architecture Search on Acoustic Scene Classification, <strong>InterSpeech20</strong></li>
  <li>Accurate and efficient single image super-resolution with matrix channel attention network, <strong>ACCV20</strong></li>
  <li>STRETCH meat grinder with ICCOS, <strong>IEEE Transactions on Plasma Science</strong></li>
  <li>Comparisons of three inductive pulse power supplies, <strong>IEEE Transactions on Plasma Science</strong></li>
  <li>FastPillars: A Deployment-friendly Pillar-based 3D Detector</li>
  <li>LogicalDefender: Discovering, Extracting, and Utilizing Common-Sense Knowledge</li>
</ol>
